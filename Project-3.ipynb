{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'coe379L-sp25/datasets/unit03/Project3/no damage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Get all image files from damaged and undamaged directories\u001b[39;00m\n\u001b[1;32m     59\u001b[0m damage_files \u001b[38;5;241m=\u001b[39m filter_images_files(damage_dir)\n\u001b[0;32m---> 60\u001b[0m no_damage_files \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_images_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_damaged_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Combine all image files and shuffle them\u001b[39;00m\n\u001b[1;32m     63\u001b[0m all_files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(damage_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m damage_files] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     64\u001b[0m     [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(no_damaged_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m no_damage_files]\n",
      "Cell \u001b[0;32mIn[15], line 55\u001b[0m, in \u001b[0;36mfilter_images_files\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_images_files\u001b[39m(base_dir):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Filter out non-image files from the dataset.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     all_files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_files \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'coe379L-sp25/datasets/unit03/Project3/no damage'"
     ]
    }
   ],
   "source": [
    "# Project 3\n",
    "# Oneza Vhora and Shalini Sundar\n",
    "\n",
    "#Project Description: You are given a dataset which contains satellite images from Texas after Hurricane Harvey. There are damaged and non-damaged building images organized into respective folders.\n",
    "#Your goal is to build multiple neural networks based on different architectures to classify images as containing buildings that are either damaged or not damaged. You will evaluate each of the networks you develop and produce and select the “best” network to “deploy”. Note that this is a binary classification problem, where the goal it to classify whether the structure in the image has damage or does not have damage.\n",
    "#Part 1: (3 points) Data preprocessing and visualization\n",
    "#You will need to perform data analysis and pre-processing to prepare the images for training. At a minimum, you should:\n",
    "#Write code to load the data into Python data structures\n",
    "#Investigate the datasets to determine basic attributes of the images\n",
    "#Ensure data is split for training, validation and testing and perform any additional preprocessing (e.g., rescaling, normalization, etc.) so that it can be used for training/evaluation of the neural networks you will build in Part 2.\n",
    "#Part 2: (10 points) Model design, training and evaluation\n",
    "#You will explore different model architectures that we have seen in class, including:\n",
    "#A dense (i.e., fully connected) ANN\n",
    "#The Lenet-5 CNN architecture\n",
    "#Alternate-Lenet-5 CNN architecture, described in the following paper (Table 1, Page 12 of the research paper https://arxiv.org/pdf/1807.01688.pdf, but note that the dataset is not the same as that analyzed in the paper.)\n",
    "#You are free to experiment with different variants on all three architectures above. For example, for the fully connected ANN, feel free to experiment with different numbers of layers and perceptrons. Train and evaluate each model you build,and select the “best” performing model.\n",
    "#Note that the input and output dimensions are fixed, as the inputs (images) and the outputs (labels) have been given. These have important implications for your architecture. Make sure you understand the constraints these impose before beginning to design and implement your networks. Failure to implement these correctly will lead to incorrect architectures and significant penalty on the project grade.\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "# Define the base directory for the dataset\n",
    "base_dir = 'coe379L-sp25/datasets/unit03/Project3'\n",
    "\n",
    "# Verify the directory structure\n",
    "damage_dir = os.path.join(base_dir, 'damage')\n",
    "no_damaged_dir = os.path.join(base_dir, 'no damage')\n",
    "\n",
    "# Make sure directories are clean before running the code\n",
    "try :\n",
    "    shutil.rmtree('train')\n",
    "    shutil.rmtree('test')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create directories for training and testing. Ensure directories exist.\n",
    "Path('train').mkdir(parents=True, exist_ok=True)\n",
    "Path('test').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "def filter_images_files(base_dir):\n",
    "    \"\"\"Filter out non-image files from the dataset.\"\"\"\n",
    "    all_files = os.listdir(base_dir)\n",
    "    return [f for f in all_files if f.endswith(('.jpg', '.jpeg', '.png'))]  \n",
    "\n",
    "# Get all image files from damaged and undamaged directories\n",
    "damage_files = filter_images_files(damage_dir)\n",
    "no_damage_files = filter_images_files(no_damaged_dir)\n",
    "\n",
    "# Combine all image files and shuffle them\n",
    "all_files = [os.path.join(damage_dir, f) for f in damage_files] + \\\n",
    "    [os.path.join(no_damaged_dir, f) for f in no_damage_files]\n",
    "random.shuffle(all_files)\n",
    "\n",
    "#split\n",
    "split_idx = int(len(all_files) * 0.8)\n",
    "train_files = all_files[:split_idx]\n",
    "test_files = all_files[split_idx:]\n",
    "\n",
    "# copy files to train and test directories\n",
    "for f in train_files:\n",
    "    shutil.copy(f, 'train/')\n",
    "for f in test_files:\n",
    "    shutil.copy(f, 'test/')\n",
    "\n",
    "# log the number of images in each directory\n",
    "print(f\"Total images: {sum(len(all_files))}\")\n",
    "print(f\"Training images: {sum(len(train_files))}\")\n",
    "print(f\"Testing images: {sum(len(test_files))}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    \"train\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    \"train\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    \"test\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Single output neuron for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10,  # Adjust the number of epochs as needed\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
